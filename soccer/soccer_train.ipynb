{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe123df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# matplotlib 설정\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# GPU를 사용할 경우\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7535fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0431304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25. 10.  0.  0. 10.  2.  0.  0. 40.  2.  0.  0.] {'winner': None}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFaCAYAAAA0D6bSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmAUlEQVR4nO3df3SU1b3v8c8kk0x+MRMTyAypBOnRFiKgFhSm2tMqkYjR1kPsrR6KseXqlROokEqVU0Wrx4ZF1zpWW4TW24JeoZxyz6FVqlgaKtZD+BWLRZCIFW8QmARIkwk/MpNk9v3DMnbKz5Ahsyd5v9Z61iLP3s8z3yc7yXzYz49xGGOMAAAALJKS6AIAAAD+HgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFgnoQFl4cKFuuSSS5SRkaFx48Zp8+bNiSwHAABYImEB5T/+4z9UVVWlRx99VG+99ZauuOIKlZaWqqmpKVElAQAASzgS9WGB48aN09VXX60f//jHkqRIJKIhQ4Zo5syZeuihhxJREgAAsIQzES8aDodVV1enuXPnRtelpKSopKREtbW1J/UPhUIKhULRryORiJqbm5Wfny+Hw9ErNQMAgJ4xxqitrU2FhYVKSTnzSZyEBJRDhw6pq6tLXq83Zr3X69WuXbtO6l9dXa3vfe97vVUeAAC4gPbu3auLL774jH0SElC6a+7cuaqqqop+3draqqKiIt3x0h1Kz05PYGUAAOBchY+GteLLKzRgwICz9k1IQBk4cKBSU1PV2NgYs76xsVE+n++k/i6XSy6X66T16dnpBBQAAJLMuVyekZC7eNLT0zVmzBjV1NRE10UiEdXU1Mjv9yeiJAAAYJGEneKpqqpSRUWFxo4dq2uuuUY//OEPdfToUX3jG99IVEkAAMASCQsoX/va13Tw4EHNmzdPgUBAV155pdasWXPShbMAAKD/SehFsjNmzNCMGTMSWQIAALAQn8UDAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsE63A8obb7yhW2+9VYWFhXI4HPrVr34V026M0bx58zR48GBlZmaqpKREu3fvjunT3NysKVOmyO12Kzc3V9OmTdORI0d6dCAAAKDv6HZAOXr0qK644gotXLjwlO0LFizQM888o8WLF2vTpk3Kzs5WaWmp2tvbo32mTJmiHTt2aO3atVq9erXeeOMN3Xvvved/FAAAoE9xGGPMeW/scGjVqlW67bbbJH08e1JYWKhvf/vbeuCBByRJra2t8nq9Wrp0qe644w69++67Ki4u1pYtWzR27FhJ0po1a3TzzTfro48+UmFh4VlfNxgMyuPx6K6au5SenX6+5QMAgF4UPhrWCxNeUGtrq9xu9xn7xvUalD179igQCKikpCS6zuPxaNy4caqtrZUk1dbWKjc3NxpOJKmkpEQpKSnatGnTKfcbCoUUDAZjFgAA0HfFNaAEAgFJktfrjVnv9XqjbYFAQAUFBTHtTqdTeXl50T5/r7q6Wh6PJ7oMGTIknmUDAADLJMVdPHPnzlVra2t02bt3b6JLAgAAF1BcA4rP55MkNTY2xqxvbGyMtvl8PjU1NcW0d3Z2qrm5Odrn77lcLrnd7pgFAAD0XXENKMOGDZPP51NNTU10XTAY1KZNm+T3+yVJfr9fLS0tqquri/ZZt26dIpGIxo0bF89yAABAknJ2d4MjR47o/fffj369Z88ebdu2TXl5eSoqKtKsWbP0b//2b7rssss0bNgwPfLIIyosLIze6TNixAjddNNNuueee7R48WJ1dHRoxowZuuOOO87pDh4AAND3dTugbN26Vddff33066qqKklSRUWFli5dqu985zs6evSo7r33XrW0tOi6667TmjVrlJGREd1m2bJlmjFjhiZMmKCUlBSVl5frmWeeicPhAACAvqBHz0FJFJ6DAgBA8knYc1AAAADigYACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDrdCijV1dW6+uqrNWDAABUUFOi2225TfX19TJ/29nZVVlYqPz9fOTk5Ki8vV2NjY0yfhoYGlZWVKSsrSwUFBZozZ446Ozt7fjQAAKBP6FZAWb9+vSorK7Vx40atXbtWHR0dmjhxoo4ePRrtM3v2bL388stauXKl1q9fr/3792vy5MnR9q6uLpWVlSkcDmvDhg16/vnntXTpUs2bNy9+RwUAAJKawxhjznfjgwcPqqCgQOvXr9c//uM/qrW1VYMGDdLy5ct1++23S5J27dqlESNGqLa2VuPHj9err76qW265Rfv375fX65UkLV68WA8++KAOHjyo9PT0s75uMBiUx+PRXTV3KT377P0BAEDihY+G9cKEF9Ta2iq3233Gvj26BqW1tVWSlJeXJ0mqq6tTR0eHSkpKon2GDx+uoqIi1dbWSpJqa2s1atSoaDiRpNLSUgWDQe3YseOUrxMKhRQMBmMWAADQd513QIlEIpo1a5auvfZajRw5UpIUCASUnp6u3NzcmL5er1eBQCDa52/DyYn2E22nUl1dLY/HE12GDBlyvmUDAIAkcN4BpbKyUu+8845WrFgRz3pOae7cuWptbY0ue/fuveCvCQAAEsd5PhvNmDFDq1ev1htvvKGLL744ut7n8ykcDqulpSVmFqWxsVE+ny/aZ/PmzTH7O3GXz4k+f8/lcsnlcp1PqQAAIAl1awbFGKMZM2Zo1apVWrdunYYNGxbTPmbMGKWlpammpia6rr6+Xg0NDfL7/ZIkv9+v7du3q6mpKdpn7dq1crvdKi4u7smxAACAPqJbMyiVlZVavny5fv3rX2vAgAHRa0Y8Ho8yMzPl8Xg0bdo0VVVVKS8vT263WzNnzpTf79f48eMlSRMnTlRxcbGmTp2qBQsWKBAI6OGHH1ZlZSWzJAAAQFI3A8qiRYskSV/60pdi1i9ZskR33323JOmpp55SSkqKysvLFQqFVFpaqmeffTbaNzU1VatXr9b06dPl9/uVnZ2tiooKPf744z07EgAA0Gf06DkoicJzUAAASD699hwUAACAC4GAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCdbgWURYsWafTo0XK73XK73fL7/Xr11Vej7e3t7aqsrFR+fr5ycnJUXl6uxsbGmH00NDSorKxMWVlZKigo0Jw5c9TZ2RmfowEAAH1CtwLKxRdfrPnz56uurk5bt27VDTfcoK985SvasWOHJGn27Nl6+eWXtXLlSq1fv1779+/X5MmTo9t3dXWprKxM4XBYGzZs0PPPP6+lS5dq3rx58T0qAACQ1BzGGNOTHeTl5ekHP/iBbr/9dg0aNEjLly/X7bffLknatWuXRowYodraWo0fP16vvvqqbrnlFu3fv19er1eStHjxYj344IM6ePCg0tPTz+k1g8GgPB6P7qq5S+nZ57YNAABIrPDRsF6Y8IJaW1vldrvP2Pe8r0Hp6urSihUrdPToUfn9ftXV1amjo0MlJSXRPsOHD1dRUZFqa2slSbW1tRo1alQ0nEhSaWmpgsFgdBbmVEKhkILBYMwCAAD6rm4HlO3btysnJ0cul0v33XefVq1apeLiYgUCAaWnpys3Nzemv9frVSAQkCQFAoGYcHKi/UTb6VRXV8vj8USXIUOGdLdsAACQRLodUD772c9q27Zt2rRpk6ZPn66Kigrt3LnzQtQWNXfuXLW2tkaXvXv3XtDXAwAAieXs7gbp6em69NJLJUljxozRli1b9PTTT+trX/uawuGwWlpaYmZRGhsb5fP5JEk+n0+bN2+O2d+Ju3xO9DkVl8sll8vV3VIBAECS6vFzUCKRiEKhkMaMGaO0tDTV1NRE2+rr69XQ0CC/3y9J8vv92r59u5qamqJ91q5dK7fbreLi4p6WAgAA+ohuzaDMnTtXkyZNUlFRkdra2rR8+XK9/vrreu211+TxeDRt2jRVVVUpLy9PbrdbM2fOlN/v1/jx4yVJEydOVHFxsaZOnaoFCxYoEAjo4YcfVmVlJTMkAAAgqlsBpampSXfddZcOHDggj8ej0aNH67XXXtONN94oSXrqqaeUkpKi8vJyhUIhlZaW6tlnn41un5qaqtWrV2v69Ony+/3Kzs5WRUWFHn/88fgeFQAASGo9fg5KIvAcFAAAkk+vPAcFAADgQiGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAVnE6nMp35Se6DAAJ5kx0AQCQ6kjVyNyRutF3ozJTM5XtzFZbR5t2BXfpD01/UFOoKdElAuhlPZpBmT9/vhwOh2bNmhVd197ersrKSuXn5ysnJ0fl5eVqbGyM2a6hoUFlZWXKyspSQUGB5syZo87Ozp6UAiBJpShFX/7UlzX9sun6rPuzKsouUr4rX5fkXKKbCm/SzM/OVGFmYaLLBNDLzjugbNmyRT/5yU80evTomPWzZ8/Wyy+/rJUrV2r9+vXav3+/Jk+eHG3v6upSWVmZwuGwNmzYoOeff15Lly7VvHnzzv8oACStwqxCfcn7JaU6Uk/Z7sv06QbvDUrhjDTQr5zXb/yRI0c0ZcoUPffcc7roooui61tbW/Wzn/1M//7v/64bbrhBY8aM0ZIlS7RhwwZt3LhRkvTb3/5WO3fu1Isvvqgrr7xSkyZN0hNPPKGFCxcqHA7H56gAJI3LPZcry5l1xj5XXHSF0lPSe6kiADY4r4BSWVmpsrIylZSUxKyvq6tTR0dHzPrhw4erqKhItbW1kqTa2lqNGjVKXq832qe0tFTBYFA7duw45euFQiEFg8GYBUDyS1GKCjIKztovLSWNC2eBfqbbF8muWLFCb731lrZs2XJSWyAQUHp6unJzc2PWe71eBQKBaJ+/DScn2k+0nUp1dbW+973vdbdUAJYzMjreefys/SImovau9l6oCIAtujWDsnfvXt1///1atmyZMjIyLlRNJ5k7d65aW1ujy969e3vttQFcOEZGwc6gjDFn7NdpOnW082gvVQXABt0KKHV1dWpqatLnPvc5OZ1OOZ1OrV+/Xs8884ycTqe8Xq/C4bBaWlpitmtsbJTP55Mk+Xy+k+7qOfH1iT5/z+Vyye12xywA+oa6w3U60nnktO3GGNUeqlUoEurFqgAkWrcCyoQJE7R9+3Zt27YtuowdO1ZTpkyJ/jstLU01NTXRberr69XQ0CC/3y9J8vv92r59u5qaPnmuwdq1a+V2u1VcXBynwwKQLJrDzVq1d5Xau9pPmkmJmIh2t+3WusA6GZ15lgVA39Kta1AGDBigkSNHxqzLzs5Wfn5+dP20adNUVVWlvLw8ud1uzZw5U36/X+PHj5ckTZw4UcXFxZo6daoWLFigQCCghx9+WJWVlXK5XHE6LADJwsjozYNv6oMjH+h67/XKcmYpLz1PTaEm1Qfrte0v2zi9A/RDcX+S7FNPPaWUlBSVl5crFAqptLRUzz77bLQ9NTVVq1ev1vTp0+X3+5Wdna2Kigo9/vjj8S4FQJIwMtp3fJ9e/PBFOeRQRmqGjned/eJZAH2Xw5zt6jQLBYNBeTwe3VVzl9KzeTYCAADJIHw0rBcmvKDW1tazXk/KoxkBAIB1CCgAAMA6ffrTjF0pLo3NH2vtZ3hEFFHd4Tq1R3gAFQDgzC4bcJl8Gad+HIcddkjaoC6Toq3NwxSOpPVob306oOQ4c/TPl/yztZ/h0RHpUH2wXu0hAgoA4Mz8A/36QsEXEl3GGfxY0ptq73JqZ+unehxQ7JxaAAAA/RoBBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgnT79oDYpIqlVUpoixqH2rp49NCbeOiIdSsLPagQAJEBHpEPHOo8luozTykhtV4ojfvvr4wFln6QrJHXpcGiAfvDuLeqM2DVpdKTzSKJLAAAkgVV7V2n1vtWJLuOU0lK69GDxL5Xnit8++3hA6ZLUJKlLRu0KdgTVZVITXRQAAN3WHmm39rPb0lI6ZRTf/3DbNZ0AAAAgAgoAALAQAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHX6TUDhea0AACSPfhNQ4vj0XQAAcIH1m4ACAACSBwEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANbpNwGFTzMGACB59JuAwqcZAwCQPPpNQAEAAMmjWwHlsccek8PhiFmGDx8ebW9vb1dlZaXy8/OVk5Oj8vJyNTY2xuyjoaFBZWVlysrKUkFBgebMmaPOzs74HA0AAOgTnN3d4PLLL9fvfve7T3bg/GQXs2fP1m9+8xutXLlSHo9HM2bM0OTJk/Xf//3fkqSuri6VlZXJ5/Npw4YNOnDggO666y6lpaXp+9//fhwOBwAA9AXdDihOp1M+n++k9a2trfrZz36m5cuX64YbbpAkLVmyRCNGjNDGjRs1fvx4/fa3v9XOnTv1u9/9Tl6vV1deeaWeeOIJPfjgg3rssceUnp7e8yMCAABJr9vXoOzevVuFhYX69Kc/rSlTpqihoUGSVFdXp46ODpWUlET7Dh8+XEVFRaqtrZUk1dbWatSoUfJ6vdE+paWlCgaD2rFjx2lfMxQKKRgMxiwAAKDv6lZAGTdunJYuXao1a9Zo0aJF2rNnj77whS+ora1NgUBA6enpys3NjdnG6/UqEAhIkgKBQEw4OdF+ou10qqur5fF4osuQIUO6UzYAAEgy3TrFM2nSpOi/R48erXHjxmno0KH65S9/qczMzLgXd8LcuXNVVVUV/ToYDBJSAADow3p0m3Fubq4+85nP6P3335fP51M4HFZLS0tMn8bGxug1Kz6f76S7ek58farrWk5wuVxyu90xCwAA6Lt6FFCOHDmiP//5zxo8eLDGjBmjtLQ01dTURNvr6+vV0NAgv98vSfL7/dq+fbuampqifdauXSu3263i4uKelHJWPEkWAIDk0a1TPA888IBuvfVWDR06VPv379ejjz6q1NRU3XnnnfJ4PJo2bZqqqqqUl5cnt9utmTNnyu/3a/z48ZKkiRMnqri4WFOnTtWCBQsUCAT08MMPq7KyUi6X64Ic4Ak8SRYAgOTRrYDy0Ucf6c4779Thw4c1aNAgXXfdddq4caMGDRokSXrqqaeUkpKi8vJyhUIhlZaW6tlnn41un5qaqtWrV2v69Ony+/3Kzs5WRUWFHn/88fge1SkwgwIAQPLoVkBZsWLFGdszMjK0cOFCLVy48LR9hg4dqldeeaU7LxsXzKAAAJA8+s1n8TCDAgBA8ug3AYUZFAAAkke/CSgAACB5EFAAAIB1+k1A4RoUAACSR78JKFyDAgBA8ug3AQUAACSPfhNQOMUDAEDy6DcBhVM8AAAkj34TUJhBAQAgefSbgMIMCgAAyaPfBBRcGMZ8vACAtfhDlZS69WGByYwfzfgxRjr650vVeSxLkkNH3r9MOZe+J0lKcweVNfRDOZiyApAoxijzeJcue/+IJOmilg6ldUTUNMglSfpgWLaO5DjFHyq79ZuAwo9hz50IJo01N6rt3WJFwhnRtqbflUqSUjOPyjPqTxp0fY2yiv4fv/8Aes9fg8nYt1pU8vsmDT7Qfsq//YECl17/4iBtGnsRQcVi/SagoGc62nLUsHyq2nYVKxLKOG2/ruPZat7sV+v20fKMfltD/scvlJrZ3ouVAuiXjNGod4K6fdU+DQ6cOpic4GsK6Y6VH+n69Qf10s2DtfnqiwgpFuo3AYVTPOevoy1HH/78HrXVF5/zNl3Hs9W8ya9IR5qGTnmBkALgwvlrOPnm8x8q51jXOW/mbQpp6i8a5JC0iZBinX5zkSw/dufHGOmjlXeorX7EeWztUMtbY3XgN7dyfRqAC2bQobC+8UL3wskJGaGIvv6LBhXtPX4BKkNP9JuAgvNzrGGoWrdfofOPeA41b/YrfHhgPMsCgI8Zoxteb1LO0e6HkxNcoYhurGnkTh/LEFBwWsZITTUTFQm5erSfziM5Orj+en73AcTdoENhjd/c3KNZcoekK//UyiyKZQgoOK3je4eo9U+j1fMTZA41b/Ir3JwXj7IAIOr69QeV3YPZkxNcoYhK1jXFoSLECwEFpxXpcCkS7tnsyQldxzNlOvvNNdkAeknOkc64XGPokDSgrSMOe0K8JPU7RqQzokhn5PTtqZ+0GWNkOiOKGC6XPVemK77fKxNxnHG8AKC7HHE8d+wwkumMcNfneYikRGKu4Yl0nfr9uTvvAUkdUF76ny/JkXL6N9G83Ig++60upTmlg4eP6j9//Ct19nwmsN8YmPuA3KnxuQMq0uXQm/Na1RL8zzjsDQCk/FSn7r+oUFJaXPaXVd+i2rtf0r4OZlK6K80pDf/WMeXlSqFwp/7v06+ote3kkzQmcu7xz2FM8l26GAwG5fF4El1Gn5edfa0+85k/yBGHZwNEImG9++5IhUK741AZAHzshaFDNTU/Py77ei0Y1E3vvx+XfeHMWltb5Xa7z9iHa1AAAIB1CCg4rc7ORnV07I/LvsLhP6urKxiXfQHACW8fP65IHE4EGGP0x2PH4lAR4oWAgtMKhd5Xc/My9fQsoDERHTr0U3V2NsapMgD42LLmZu0OhXq8n30dHXru0KE4VIR4IaDgjA4ffq7Hsyih0Htqbv5FnCoCgE8EOjv13KFDPZpFMcboxeZmfRAOx7Ey9BQBBWf08SzK/5Ex53f7kzEdOnjwJ8yeALhgljU369329vOa7TXG6MNwWP+b2RPrEFBwVgcOPKaDB3/U7ZBiTIcOHPieDh5ceIEqA4CPZ1Fu/+AD7TyPkPJhOKyv7tmjPzN7Yh0CCs7KmJD27XtI+/d/V6HQB2cNKsZ06vjxHfroo9lqbFwgiWcKALiwdoVCun3PHi3/y190LBI5a1Bpj0T0n3/5i766Z4/quDjWSjwHBd3idBYoP/9uDRz4v+R0FkiSHI50GfPx/z7C4f+nQ4cWqbl5hbq6DieyVAD9UJrDoVEZGZrt9eq2v75PpDocckjq/Ovb3ZpgUD9satLWY8cUSr63wD7hXJ6DQkDBeXE6B8nh+PhzerKzx+vo0Y2SpEjkOMEEQMKlORwqcDrlkORLS1OGw6EP/3oa52BnJ8EkwQgoAADAOjxJFgAAJCUCCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdZIyoCTho1sAAMBfncv7eFIGlMOHeVIpAADJqq2t7ax9nL1QR9zl5eVJkhoaGniirAWCwaCGDBmivXv3nvXJgLiwGAt7MBb2YCzsYYxRW1ubCgsLz9o3KQNKSsrHEz8ej4cfNou43W7GwxKMhT0YC3swFnY414mFpDzFAwAA+jYCCgAAsE5SBhSXy6VHH31ULpcr0aVAjIdNGAt7MBb2YCySk8Nwzy4AALBMUs6gAACAvo2AAgAArENAAQAA1iGgAAAA6xBQAACAdZIyoCxcuFCXXHKJMjIyNG7cOG3evDnRJfUp1dXVuvrqqzVgwAAVFBTotttuU319fUyf9vZ2VVZWKj8/Xzk5OSovL1djY2NMn4aGBpWVlSkrK0sFBQWaM2eOOjs7e/NQ+pz58+fL4XBo1qxZ0XWMRe/at2+fvv71rys/P1+ZmZkaNWqUtm7dGm03xmjevHkaPHiwMjMzVVJSot27d8fso7m5WVOmTJHb7VZubq6mTZumI0eO9PahJLWuri498sgjGjZsmDIzM/UP//APeuKJJ2I+hI6xSHImyaxYscKkp6ebn//852bHjh3mnnvuMbm5uaaxsTHRpfUZpaWlZsmSJeadd94x27ZtMzfffLMpKioyR44cifa57777zJAhQ0xNTY3ZunWrGT9+vPn85z8fbe/s7DQjR440JSUl5o9//KN55ZVXzMCBA83cuXMTcUh9wubNm80ll1xiRo8ebe6///7oesai9zQ3N5uhQ4eau+++22zatMl88MEH5rXXXjPvv/9+tM/8+fONx+Mxv/rVr8zbb79tvvzlL5thw4aZ48ePR/vcdNNN5oorrjAbN240f/jDH8yll15q7rzzzkQcUtJ68sknTX5+vlm9erXZs2ePWblypcnJyTFPP/10tA9jkdySLqBcc801prKyMvp1V1eXKSwsNNXV1Qmsqm9ramoyksz69euNMca0tLSYtLQ0s3Llymifd99910gytbW1xhhjXnnlFZOSkmICgUC0z6JFi4zb7TahUKh3D6APaGtrM5dddplZu3at+eIXvxgNKIxF73rwwQfNddddd9r2SCRifD6f+cEPfhBd19LSYlwul/nFL35hjDFm586dRpLZsmVLtM+rr75qHA6H2bdv34Urvo8pKysz3/zmN2PWTZ482UyZMsUYw1j0BUl1iiccDquurk4lJSXRdSkpKSopKVFtbW0CK+vbWltbJX3yKdJ1dXXq6OiIGYfhw4erqKgoOg61tbUaNWqUvF5vtE9paamCwaB27NjRi9X3DZWVlSorK4v5nkuMRW976aWXNHbsWH31q19VQUGBrrrqKj333HPR9j179igQCMSMh8fj0bhx42LGIzc3V2PHjo32KSkpUUpKijZt2tR7B5PkPv/5z6umpkbvvfeeJOntt9/Wm2++qUmTJkliLPqCpPo040OHDqmrqyvmD60keb1e7dq1K0FV9W2RSESzZs3Stddeq5EjR0qSAoGA0tPTlZubG9PX6/UqEAhE+5xqnE604dytWLFCb731lrZs2XJSG2PRuz744AMtWrRIVVVV+td//Vdt2bJF3/rWt5Senq6Kioro9/NU3++/HY+CgoKYdqfTqby8PMajGx566CEFg0ENHz5cqamp6urq0pNPPqkpU6ZIEmPRByRVQEHvq6ys1DvvvKM333wz0aX0S3v37tX999+vtWvXKiMjI9Hl9HuRSERjx47V97//fUnSVVddpXfeeUeLFy9WRUVFgqvrX375y19q2bJlWr58uS6//HJt27ZNs2bNUmFhIWPRRyTVKZ6BAwcqNTX1pDsUGhsb5fP5ElRV3zVjxgytXr1av//973XxxRdH1/t8PoXDYbW0tMT0/9tx8Pl8pxynE204N3V1dWpqatLnPvc5OZ1OOZ1OrV+/Xs8884ycTqe8Xi9j0YsGDx6s4uLimHUjRoxQQ0ODpE++n2f6G+Xz+dTU1BTT3tnZqebmZsajG+bMmaOHHnpId9xxh0aNGqWpU6dq9uzZqq6ulsRY9AVJFVDS09M1ZswY1dTURNdFIhHV1NTI7/cnsLK+xRijGTNmaNWqVVq3bp2GDRsW0z5mzBilpaXFjEN9fb0aGhqi4+D3+7V9+/aYX/61a9fK7Xaf9AcepzdhwgRt375d27Ztiy5jx47VlClTov9mLHrPtddee9It9++9956GDh0qSRo2bJh8Pl/MeASDQW3atClmPFpaWlRXVxfts27dOkUiEY0bN64XjqJvOHbsmFJSYt/CUlNTFYlEJDEWfUKir9LtrhUrVhiXy2WWLl1qdu7cae69916Tm5sbc4cCemb69OnG4/GY119/3Rw4cCC6HDt2LNrnvvvuM0VFRWbdunVm69atxu/3G7/fH20/cWvrxIkTzbZt28yaNWvMoEGDuLU1Dv72Lh5jGIvetHnzZuN0Os2TTz5pdu/ebZYtW2aysrLMiy++GO0zf/58k5uba37961+bP/3pT+YrX/nKKW9tveqqq8ymTZvMm2++aS677DJube2miooK86lPfSp6m/F//dd/mYEDB5rvfOc70T6MRXJLuoBijDE/+tGPTFFRkUlPTzfXXHON2bhxY6JL6lMknXJZsmRJtM/x48fNv/zLv5iLLrrIZGVlmX/6p38yBw4ciNnPhx9+aCZNmmQyMzPNwIEDzbe//W3T0dHRy0fT9/x9QGEsetfLL79sRo4caVwulxk+fLj56U9/GtMeiUTMI488Yrxer3G5XGbChAmmvr4+ps/hw4fNnXfeaXJycozb7Tbf+MY3TFtbW28eRtILBoPm/vvvN0VFRSYjI8N8+tOfNt/97ndjbp1nLJKbw5i/eeweAACABZLqGhQAANA/EFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDr/H4D+t/Or+xkpAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import soccer_env\n",
    "\n",
    "env = gym.make(\"SoccerEnv\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.FlattenObservation(env)\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "frame = env.render()\n",
    "plt.imshow(frame)\n",
    "\n",
    "print(state, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a2f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # 최적화 중에 다음 행동을 결정하기 위해서 하나의 요소 또는 배치를 이용해 호촐됩니다.\n",
    "    # ([[left0exp,right0exp]...]) 를 반환합니다.\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "08dc7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE는 리플레이 버퍼에서 샘플링된 트랜지션의 수입니다.\n",
    "# GAMMA는 이전 섹션에서 언급한 할인 계수입니다.\n",
    "# EPS_START는 엡실론의 시작 값입니다.\n",
    "# EPS_END는 엡실론의 최종 값입니다.\n",
    "# EPS_DECAY는 엡실론의 지수 감쇠(exponential decay) 속도 제어하며, 높을수록 감쇠 속도가 느립니다.\n",
    "# TAU는 목표 네트워크의 업데이트 속도입니다.\n",
    "# LR은 ``AdamW`` 옵티마이저의 학습율(learning rate)입니다.\n",
    "BATCH_SIZE = 16\n",
    "GAMMA = 0.8\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-2\n",
    "\n",
    "class ModelSettings:\n",
    "    def __init__(self, model, n_observations, action_space):\n",
    "        self.action_space = action_space\n",
    "        n_actions = action_space.n\n",
    "\n",
    "        self.policy_net = model(n_observations, n_actions).to(device)\n",
    "        self.target_net = model(n_observations, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "        self.memory = ReplayMemory(10000)\n",
    "\n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
    "                # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
    "                # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[self.action_space.sample()]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\n",
    "        # 전환합니다.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다\n",
    "        # (최종 상태는 시뮬레이션이 종료 된 이후의 상태)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
    "        # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
    "        # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
    "        # max(1).values로 최고의 보상을 선택하십시오.\n",
    "        # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "        # 기대 Q 값 계산\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        # Huber 손실 계산\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 모델 최적화\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 변화도 클리핑 바꿔치기\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        target_net_state_dict = self.target_net.state_dict()\n",
    "        policy_net_state_dict = self.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "setting1 = ModelSettings(DQN, n_observations, env.action_space[0])\n",
    "setting2 = ModelSettings(DQN, n_observations, env.action_space[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "612eb6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "42229887",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# (정책 네트워크에서) 최적화 한단계 수행\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43msetting1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m setting2\u001b[38;5;241m.\u001b[39moptimize_model()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 목표 네트워크의 가중치를 소프트 업데이트\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[148], line 83\u001b[0m, in \u001b[0;36mModelSettings.optimize_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# 모델 최적화\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 83\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# 변화도 클리핑 바꿔치기\u001b[39;00m\n\u001b[0;32m     85\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shhon\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shhon\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # 환경과 상태 초기화\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action1 = setting1.select_action(state)\n",
    "        action2 = setting2.select_action(state)\n",
    "        observation, reward, terminated, truncated, info = env.step((action1.item(), action2.item()))\n",
    "\n",
    "        #frame = env.render()\n",
    "        #frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        #cv2.imshow('Env', frame_bgr)\n",
    "        #cv2.waitKey(1)\n",
    "\n",
    "        if t == 100:\n",
    "            truncated = True\n",
    "\n",
    "        d1 = -torch.norm(state[0][0:2] - state[0][4:6])\n",
    "        d2 = -torch.norm(state[0][0:2] - state[0][8:10])\n",
    "\n",
    "        reward1 = int(info[\"winner\"] == \"Player1\")\n",
    "        reward2 = int(info[\"winner\"] == \"Player2\")\n",
    "        reward1, reward2 = (reward1 - reward2, reward2 - reward1)\n",
    "        reward1 += -d1\n",
    "        reward2 += -d2\n",
    "        reward = torch.tensor([reward1, reward2], device=device)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # 메모리에 변이 저장\n",
    "        setting1.memory.push(state, action1, next_state, reward[0].view(1, 1))\n",
    "        setting2.memory.push(state, action2, next_state, reward[1].view(1, 1))\n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "        # (정책 네트워크에서) 최적화 한단계 수행\n",
    "        setting1.optimize_model()\n",
    "        setting2.optimize_model()\n",
    "\n",
    "        # 목표 네트워크의 가중치를 소프트 업데이트\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        setting1.update_target_net()\n",
    "        setting2.update_target_net()\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54d3a4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b17bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
