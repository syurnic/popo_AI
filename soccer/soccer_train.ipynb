{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f269ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install box2d\n",
    "%pip install gymnasium\n",
    "%pip install opencv-python==4.10.0.84\n",
    "!apt-get update && apt-get install -y libgl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe123df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# matplotlib 설정\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# GPU를 사용할 경우\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7535fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0431304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.        3.        0.        0.       30.127947  3.        0.\n",
      "  0.       19.872053  3.        0.        0.      ] {'winner': None}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFaCAYAAAA0D6bSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmMElEQVR4nO3df3SU1Z3H8c9Mkpn8YiYmkBkiBHBFIQJqQcNUu62SkmJq6xJ7KqWQtrQe2UCFtFTZKrZaGw49Z926RWhtC3oqy8o5a1tZRTG0WEv4FUuLUBErNkGcCUKTIUAySebuHy5Tp/IrJGTuTN6vc55zyHPv88z3yYXkw53nueMwxhgBAABYxJnoAgAAAP4RAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWCehAWX58uUaOXKkMjMzVVpaqu3btyeyHAAAYImEBZT//u//Vk1NjR544AG9+uqruvrqq1VeXq7m5uZElQQAACzhSNSHBZaWluq6667Tj370I0lSNBrV8OHDNX/+fN17772JKAkAAFgiPREvGolE1NDQoMWLF8f2OZ1OlZWVqb6+/kP9Ozo61NHREfs6Go3q6NGjKigokMPh6JeaAQBA7xhjdOzYMRUVFcnpPPubOAkJKO+99566u7vl8/ni9vt8Pr3++usf6l9bW6vvfve7/VUeAAC4iJqamjRs2LCz9klIQOmpxYsXq6amJvZ1a2uriouLdcev75Arx5XAygAAwPmKHI9o7WfWatCgQefsm5CAMnjwYKWlpSkUCsXtD4VC8vv9H+rvdrvldrs/tN+V4yKgAACQZM7n9oyEPMXjcrk0ceJE1dXVxfZFo1HV1dUpEAgkoiQAAGCRhL3FU1NTo6qqKk2aNEnXX3+9/uM//kPHjx/Xl7/85USVBAAALJGwgPL5z39ehw8f1pIlSxQMBnXNNddow4YNH7pxFgAADDwJvUl23rx5mjdvXiJLAAAAFuKzeAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADW6XFAefnll3XrrbeqqKhIDodDv/zlL+PajTFasmSJhg4dqqysLJWVlWn//v1xfY4ePaqZM2fK4/EoLy9Pc+bMUVtbW68uBAAApI4eB5Tjx4/r6quv1vLly0/bvmzZMj366KNauXKltm3bppycHJWXl6u9vT3WZ+bMmdqzZ482btyo9evX6+WXX9add9554VcBAABSisMYYy74YIdDzzzzjG677TZJ78+eFBUV6Rvf+Ia++c1vSpJaW1vl8/m0evVq3XHHHfrzn/+skpIS7dixQ5MmTZIkbdiwQbfccosOHjyooqKic75uOByW1+vV7LrZcuW4LrR8AADQjyLHI3pyypNqbW2Vx+M5a98+vQflwIEDCgaDKisri+3zer0qLS1VfX29JKm+vl55eXmxcCJJZWVlcjqd2rZt22nP29HRoXA4HLcBAIDU1acBJRgMSpJ8Pl/cfp/PF2sLBoMqLCyMa09PT1d+fn6szz+qra2V1+uNbcOHD+/LsgEAgGWS4imexYsXq7W1NbY1NTUluiQAAHAR9WlA8fv9kqRQKBS3PxQKxdr8fr+am5vj2ru6unT06NFYn3/kdrvl8XjiNgAAkLr6NKCMGjVKfr9fdXV1sX3hcFjbtm1TIBCQJAUCAbW0tKihoSHWZ9OmTYpGoyotLe3LcgAAQJJK7+kBbW1tevPNN2NfHzhwQLt27VJ+fr6Ki4u1YMECfe9739Po0aM1atQo3X///SoqKoo96TN27Fh96lOf0te+9jWtXLlSnZ2dmjdvnu64447zeoIHAACkvh4HlJ07d+qmm26KfV1TUyNJqqqq0urVq/Wtb31Lx48f15133qmWlhbdeOON2rBhgzIzM2PHPPXUU5o3b56mTJkip9OpyspKPfroo31wOQAAIBX0ah2URGEdFAAAkk/C1kEBAADoCwQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANbpUUCpra3Vddddp0GDBqmwsFC33Xab9u3bF9envb1d1dXVKigoUG5uriorKxUKheL6NDY2qqKiQtnZ2SosLNSiRYvU1dXV+6sBAAApoUcBZfPmzaqurtbWrVu1ceNGdXZ2aurUqTp+/Hisz8KFC/Xss89q3bp12rx5sw4dOqTp06fH2ru7u1VRUaFIJKItW7boiSee0OrVq7VkyZK+uyoAAJDUHMYYc6EHHz58WIWFhdq8ebP++Z//Wa2trRoyZIjWrFmj22+/XZL0+uuva+zYsaqvr9fkyZP1/PPP69Of/rQOHTokn88nSVq5cqXuueceHT58WC6X65yvGw6H5fV6Nbtutlw55+4PAAASL3I8oienPKnW1lZ5PJ6z9u3VPSitra2SpPz8fElSQ0ODOjs7VVZWFuszZswYFRcXq76+XpJUX1+v8ePHx8KJJJWXlyscDmvPnj2nfZ2Ojg6Fw+G4DQAApK4LDijRaFQLFizQDTfcoHHjxkmSgsGgXC6X8vLy4vr6fD4Fg8FYnw+Gk1Ptp9pOp7a2Vl6vN7YNHz78QssGAABJ4IIDSnV1tV577TWtXbu2L+s5rcWLF6u1tTW2NTU1XfTXBAAAiZN+IQfNmzdP69ev18svv6xhw4bF9vv9fkUiEbW0tMTNooRCIfn9/lif7du3x53v1FM+p/r8I7fbLbfbfSGlAgCAJNSjGRRjjObNm6dnnnlGmzZt0qhRo+LaJ06cqIyMDNXV1cX27du3T42NjQoEApKkQCCg3bt3q7m5OdZn48aN8ng8Kikp6c21AACAFNGjGZTq6mqtWbNGv/rVrzRo0KDYPSNer1dZWVnyer2aM2eOampqlJ+fL4/Ho/nz5ysQCGjy5MmSpKlTp6qkpESzZs3SsmXLFAwGdd9996m6uppZEgAAIKmHAWXFihWSpE984hNx+1etWqUvfelLkqRHHnlETqdTlZWV6ujoUHl5uR577LFY37S0NK1fv15z585VIBBQTk6Oqqqq9OCDD/buSgAAQMro1TooicI6KAAAJJ9+WwcFAADgYiCgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFinRwFlxYoVmjBhgjwejzwejwKBgJ5//vlYe3t7u6qrq1VQUKDc3FxVVlYqFArFnaOxsVEVFRXKzs5WYWGhFi1apK6urr65GgAAkBJ6FFCGDRumpUuXqqGhQTt37tTNN9+sz372s9qzZ48kaeHChXr22We1bt06bd68WYcOHdL06dNjx3d3d6uiokKRSERbtmzRE088odWrV2vJkiV9e1UAACCpOYwxpjcnyM/P1w9+8APdfvvtGjJkiNasWaPbb79dkvT6669r7Nixqq+v1+TJk/X888/r05/+tA4dOiSfzydJWrlype655x4dPnxYLpfrvF4zHA7L6/Vqdt1suXLO7xgAAJBYkeMRPTnlSbW2tsrj8Zy17wXfg9Ld3a21a9fq+PHjCgQCamhoUGdnp8rKymJ9xowZo+LiYtXX10uS6uvrNX78+Fg4kaTy8nKFw+HYLMzpdHR0KBwOx20AACB19Tig7N69W7m5uXK73brrrrv0zDPPqKSkRMFgUC6XS3l5eXH9fT6fgsGgJCkYDMaFk1Ptp9rOpLa2Vl6vN7YNHz68p2UDAIAk0uOAcuWVV2rXrl3atm2b5s6dq6qqKu3du/di1BazePFitba2xrampqaL+noAACCx0nt6gMvl0uWXXy5Jmjhxonbs2KEf/vCH+vznP69IJKKWlpa4WZRQKCS/3y9J8vv92r59e9z5Tj3lc6rP6bjdbrnd7p6WCgAAklSv10GJRqPq6OjQxIkTlZGRobq6uljbvn371NjYqEAgIEkKBALavXu3mpubY302btwoj8ejkpKS3pYCAABSRI9mUBYvXqxp06apuLhYx44d05o1a/Tb3/5WL7zwgrxer+bMmaOamhrl5+fL4/Fo/vz5CgQCmjx5siRp6tSpKikp0axZs7Rs2TIFg0Hdd999qq6uZoYEAADE9CigNDc3a/bs2Xr33Xfl9Xo1YcIEvfDCC/rkJz8pSXrkkUfkdDpVWVmpjo4OlZeX67HHHosdn5aWpvXr12vu3LkKBALKyclRVVWVHnzwwb69KgAAkNR6vQ5KIrAOCgAAyadf1kEBAAC4WAgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKzTq4CydOlSORwOLViwILavvb1d1dXVKigoUG5uriorKxUKheKOa2xsVEVFhbKzs1VYWKhFixapq6urN6UAAIAUcsEBZceOHfrxj3+sCRMmxO1fuHChnn32Wa1bt06bN2/WoUOHNH369Fh7d3e3KioqFIlEtGXLFj3xxBNavXq1lixZcuFXAQAAUsoFBZS2tjbNnDlTjz/+uC655JLY/tbWVv3sZz/Tv//7v+vmm2/WxIkTtWrVKm3ZskVbt26VJL344ovau3evfvGLX+iaa67RtGnT9NBDD2n58uWKRCJ9c1UAACCpXVBAqa6uVkVFhcrKyuL2NzQ0qLOzM27/mDFjVFxcrPr6eklSfX29xo8fL5/PF+tTXl6ucDisPXv2nPb1Ojo6FA6H4zYAAJC60nt6wNq1a/Xqq69qx44dH2oLBoNyuVzKy8uL2+/z+RQMBmN9PhhOTrWfajud2tpaffe73+1pqQAAIEn1aAalqalJd999t5566illZmZerJo+ZPHixWptbY1tTU1N/fbaAACg//UooDQ0NKi5uVkf+chHlJ6ervT0dG3evFmPPvqo0tPT5fP5FIlE1NLSEndcKBSS3++XJPn9/g891XPq61N9/pHb7ZbH44nbAABA6upRQJkyZYp2796tXbt2xbZJkyZp5syZsT9nZGSorq4udsy+ffvU2NioQCAgSQoEAtq9e7eam5tjfTZu3CiPx6OSkpI+uiwAAJDMenQPyqBBgzRu3Li4fTk5OSooKIjtnzNnjmpqapSfny+Px6P58+crEAho8uTJkqSpU6eqpKREs2bN0rJlyxQMBnXfffepurpabre7jy4LAAAksx7fJHsujzzyiJxOpyorK9XR0aHy8nI99thjsfa0tDStX79ec+fOVSAQUE5OjqqqqvTggw/2dSkAACBJOYwxJtFF9FQ4HJbX69Xsutly5bgSXQ4AADgPkeMRPTnlSbW2tp7zflI+iwcAAFiHgAIAAKzT5/eg2MTtdGtSwSQ5Lc1hUUXVcKRB7dH2RJcCALDc6EGj5c88/XIcdtgjaYu6jVM7j45SJJrRq7OldEDJTc/VF0Z+QS6nnfepdEY7tS+8T+0dBBQAwNkFBgf0scKPJbqMs/iRpFfU3p2uva2X9jqg2Dm1AAAABjQCCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOim9UJsUldQqKUNR41B7d+8WjelrndFOJeFnNQIAEqAz2qkTXScSXcYZZaa1y+nou/OleEB5R9LVkrp1pGOQfvDnT6sratekUVtXW6JLAAAkgWeantH6d9YnuozTynB2656Sp5Xv7rtzpnhA6ZbULKlbRu0Kd4bVbdISXRQAAD3WHm239rPbMpxdMurb/3DbNZ0AAAAgAgoAALAQAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUGTEBhvVYAAJLHgAkofbj6LgAAuMgGTEABAADJg4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsMmIDCpxkDAJA8BkxA4dOMAQBIHgMmoAAAgOTRo4Dyne98Rw6HI24bM2ZMrL29vV3V1dUqKChQbm6uKisrFQqF4s7R2NioiooKZWdnq7CwUIsWLVJXV1ffXA0AAEgJ6T094KqrrtJLL7309xOk//0UCxcu1P/+7/9q3bp18nq9mjdvnqZPn67f//73kqTu7m5VVFTI7/dry5YtevfddzV79mxlZGTo+9//fh9cDgAASAU9Dijp6eny+/0f2t/a2qqf/exnWrNmjW6++WZJ0qpVqzR27Fht3bpVkydP1osvvqi9e/fqpZdeks/n0zXXXKOHHnpI99xzj77zne/I5XL1/ooAAEDS6/E9KPv371dRUZEuu+wyzZw5U42NjZKkhoYGdXZ2qqysLNZ3zJgxKi4uVn19vSSpvr5e48ePl8/ni/UpLy9XOBzWnj17zviaHR0dCofDcRsAAEhdPQoopaWlWr16tTZs2KAVK1bowIED+tjHPqZjx44pGAzK5XIpLy8v7hifz6dgMChJCgaDceHkVPuptjOpra2V1+uNbcOHD+9J2QAAIMn06C2eadOmxf48YcIElZaWasSIEXr66aeVlZXV58WdsnjxYtXU1MS+DofDhBQAAFJYrx4zzsvL0xVXXKE333xTfr9fkUhELS0tcX1CoVDsnhW/3/+hp3pOfX26+1pOcbvd8ng8cRsAAEhdvQoobW1t+stf/qKhQ4dq4sSJysjIUF1dXax93759amxsVCAQkCQFAgHt3r1bzc3NsT4bN26Ux+NRSUlJb0o5J1aSBQAgefToLZ5vfvObuvXWWzVixAgdOnRIDzzwgNLS0jRjxgx5vV7NmTNHNTU1ys/Pl8fj0fz58xUIBDR58mRJ0tSpU1VSUqJZs2Zp2bJlCgaDuu+++1RdXS23231RLvAUVpIFACB59CigHDx4UDNmzNCRI0c0ZMgQ3Xjjjdq6dauGDBkiSXrkkUfkdDpVWVmpjo4OlZeX67HHHosdn5aWpvXr12vu3LkKBALKyclRVVWVHnzwwb69qtNgBgUAgOTRo4Cydu3as7ZnZmZq+fLlWr58+Rn7jBgxQs8991xPXrZPMIMCAEDyGDCfxcMMCgAAyWPABBRmUAAASB49XuoegB3SO6Ma+dcTchqj7BPdyv9bRAcvfX89oqAvU2FPRoIrHJg6wx61h95fNuHkwWFy5R9RWvZJORxRZY94W84MPhwVOB8EFCDJpHdFNfb1Y/rUiyFd/pc2OU7z/mXQ59Zv/3mIdky8RMcGpUsO5hAvJmOkrmMe/W3ndTr8u4+rI3SadZ0cRrmX75fvky9o0Ji9cqZ393+hQBIZMAGFe1CQ9IzRsHdO6o51BzX6zTY5z/KXemioQ3esO6hP/O49vVBWqN8HCggpF4kx0pH6GxTaWP7/weQM32fjUNv+K9X25mjlXr5fwz63VlmXHmRYgDPgHhQgGfx/OJn7k7d05f6zh5NTHJKGBts14+mDumHLkfd/k6JPGSMd2XKjDj49Qx2hoTqvnzTGqbb9V+rA43N18p1hDAtwBgMmoADJLPtEt+782QEVvhfp8bHuSFQz1h3UlW+0XYTKBra2N8bo4Lo7FI30fKHJjsOFevvnd6r7ZPZFqAxIfgMmoPCfFCQtY1S646gKD3dc8Cnckaim1oXkiNr7L8HpcOqKQVfo6ryrdUvRLRrrGavB7sGJLuuMTNSh0EufvKBwckp7c6H+tuN6ZlGA0xgw96DwFg+SVfbJbt20+bDSor07z5VvtOmK/W3ad0WudfejDM0cqi+M/IJGDxqtNGdabP+RjiN6ufllbTi0QVH18hvQh4yR2vZfobb9V/buRNE0Hd58sy65bpvSs0/2TXFAimAGBbDc9Tv+1qvZk1Pckag+WRc6d8d+5na69dXLv6ox3jFx4USSCtwFuvXSW1U6uDRB1Z2BcShUN7VXsyenvD+LYtn1ARYYMAHFrv8vAufP29rZ69mTUy5p6TztY8mJNDF/ooqyis7Ynu5M1xT/FLmcrn6s6tw6/3ZJ35womqbOsLdvzgWkkAETUICkZEyfhmsbg7onw6N059nfbfZmeOVM8R9X3IcCxEvtf/EfwL99JCNXJKoRfz3eZ+fzhDvlC7X32fl6K92RrlG5o87ZLzMtU8U5xf1Q0flpD/nVeczTZ+c78deRMp2s/At80IAJKDb+zxE4l4g7TW+PyOmz87V6MhTyZfbZ+Xqry3Tprba3ztmvvbtdjccb+6Gi85PpCypjULjPzpc94m05XZ19dj4gFQyYgALATkc6jqgzevZfzofbD6vbsDQ8MJAMmIDCWzxIVoeHuNWV1jdzgEFfpoxl04l/avmT3j7+tswZbsKIRCOqC9Wp01g0w+AwyvQH++ZUaV1yD2nuk3MBqWTABBTLfiYD5+3Va/L0TlHv35Y5kelU3U1DrFsDJRKN6Kdv/lQ7ju5QJBpR1ERljFHURHXwxEGtfXutXj36aqLLjONwSENueknOzN6vXZJ16UHlXf2HPqgKSC0DZqE2IFm1Z6Vp0ycKNeupvyq9F48bv3aVVwdG9d39LH3paOSofvrmT+XP8iszLVNFWUX66/G/qjXSqmNdxxJd3mnljDwg71Wv6W8N113wORxpXRry8d8oLcueG5cBWwyYGRQgmf3hmjwdHJZ9wW9VHs9KU91NQ2Qsmz35ICOjd0++qwNtB/T7w7/XwRMHrQ0nkuRwGg256SWlZV3oU1ZGWcOalHeNXbNDgC0IKEASOJmVpp/MGaWm4Vk9DinHs9K0avYIvWXp7Ekyyxn1lkbMXn0BIcUoa3ijRn3lJ8yeAGdAQAGSxOEhbq386mXaMfESdaY7zhlUog5p/2U5WlU1Qn+c4LXu3pNU4HBI3gm7NGL2KuVc9qbkONd7cEaO9E5dMnGHLvvqj+Ue8l6/1Akko6S+ByXaFVW068w/EKIfWB/cGCPTFVXUtkcYgB4IXZKhx79YrOGfGKzyl5p1zZ9a5Pz/v+YO/f1ptQMjs1X3iULtKfHoZFaa1G3Es2wXj+eqPyjnstcV3nuVmn8zRSf+emrxuQ+MirNbeRN2qbDsRWUPa5IjLapoV6IqBvpW1BmNWw452n36389n+539jxzmTM/2WSwcDsvr9coz3COH88yBIz8vqm9//Zgy0qXDR5yq/dEgdbGUAlJEWrfR4PcichijvLQ0DXe5tPvk+0+VtHoz3g8m6HfdJ7PU2fr+Z+tkZY1XJNKk7u4WORxGrsHvyZHGDyGknox06dtfDys/z6gjIn3vh4PUeuzDb9KYqFG4KazW1lZ5PGdfjTmpAwoAAEg+5xNQuAcFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWCcpA4oxJtElAACAC3Q+v8eTMqAcOXIk0SUAAIALdOzYsXP2Se+HOvpcfn6+JKmxsVFerzfB1SAcDmv48OFqamqSx+NJdDkDGmNhD8bCHoyFPYwxOnbsmIqKis7ZNykDitP5/sSP1+vlL5tFPB4P42EJxsIejIU9GAs7nO/EQlK+xQMAAFIbAQUAAFgnKQOK2+3WAw88ILfbnehSIMbDJoyFPRgLezAWyclheGYXAABYJilnUAAAQGojoAAAAOsQUAAAgHUIKAAAwDoEFAAAYJ2kDCjLly/XyJEjlZmZqdLSUm3fvj3RJaWU2tpaXXfddRo0aJAKCwt12223ad++fXF92tvbVV1drYKCAuXm5qqyslKhUCiuT2NjoyoqKpSdna3CwkItWrRIXV1d/XkpKWfp0qVyOBxasGBBbB9j0b/eeecdffGLX1RBQYGysrI0fvx47dy5M9ZujNGSJUs0dOhQZWVlqaysTPv37487x9GjRzVz5kx5PB7l5eVpzpw5amtr6+9LSWrd3d26//77NWrUKGVlZemf/umf9NBDD8V9CB1jkeRMklm7dq1xuVzm5z//udmzZ4/52te+ZvLy8kwoFEp0aSmjvLzcrFq1yrz22mtm165d5pZbbjHFxcWmra0t1ueuu+4yw4cPN3V1dWbnzp1m8uTJ5qMf/Wisvaury4wbN86UlZWZP/zhD+a5554zgwcPNosXL07EJaWE7du3m5EjR5oJEyaYu+++O7afseg/R48eNSNGjDBf+tKXzLZt28xbb71lXnjhBfPmm2/G+ixdutR4vV7zy1/+0vzxj380n/nMZ8yoUaPMyZMnY30+9alPmauvvtps3brV/O53vzOXX365mTFjRiIuKWk9/PDDpqCgwKxfv94cOHDArFu3zuTm5pof/vCHsT6MRXJLuoBy/fXXm+rq6tjX3d3dpqioyNTW1iawqtTW3NxsJJnNmzcbY4xpaWkxGRkZZt26dbE+f/7zn40kU19fb4wx5rnnnjNOp9MEg8FYnxUrVhiPx2M6Ojr69wJSwLFjx8zo0aPNxo0bzcc//vFYQGEs+tc999xjbrzxxjO2R6NR4/f7zQ9+8IPYvpaWFuN2u81//dd/GWOM2bt3r5FkduzYEevz/PPPG4fDYd55552LV3yKqaioMF/5ylfi9k2fPt3MnDnTGMNYpIKkeosnEomooaFBZWVlsX1Op1NlZWWqr69PYGWprbW1VdLfP0W6oaFBnZ2dceMwZswYFRcXx8ahvr5e48ePl8/ni/UpLy9XOBzWnj17+rH61FBdXa2Kioq477nEWPS3X//615o0aZI+97nPqbCwUNdee60ef/zxWPuBAwcUDAbjxsPr9aq0tDRuPPLy8jRp0qRYn7KyMjmdTm3btq3/LibJffSjH1VdXZ3eeOMNSdIf//hHvfLKK5o2bZokxiIVJNWnGb/33nvq7u6O+0ErST6fT6+//nqCqkpt0WhUCxYs0A033KBx48ZJkoLBoFwul/Ly8uL6+nw+BYPBWJ/TjdOpNpy/tWvX6tVXX9WOHTs+1MZY9K+33npLK1asUE1Njf7t3/5NO3bs0Ne//nW5XC5VVVXFvp+n+35/cDwKCwvj2tPT05Wfn8949MC9996rcDisMWPGKC0tTd3d3Xr44Yc1c+ZMSWIsUkBSBRT0v+rqar322mt65ZVXEl3KgNTU1KS7775bGzduVGZmZqLLGfCi0agmTZqk73//+5Kka6+9Vq+99ppWrlypqqqqBFc3sDz99NN66qmntGbNGl111VXatWuXFixYoKKiIsYiRSTVWzyDBw9WWlrah55QCIVC8vv9Caoqdc2bN0/r16/Xb37zGw0bNiy23+/3KxKJqKWlJa7/B8fB7/efdpxOteH8NDQ0qLm5WR/5yEeUnp6u9PR0bd68WY8++qjS09Pl8/kYi340dOhQlZSUxO0bO3asGhsbJf39+3m2n1F+v1/Nzc1x7V1dXTp69Cjj0QOLFi3SvffeqzvuuEPjx4/XrFmztHDhQtXW1kpiLFJBUgUUl8uliRMnqq6uLrYvGo2qrq5OgUAggZWlFmOM5s2bp2eeeUabNm3SqFGj4tonTpyojIyMuHHYt2+fGhsbY+MQCAS0e/fuuH/8GzdulMfj+dAPeJzZlClTtHv3bu3atSu2TZo0STNnzoz9mbHoPzfccMOHHrl/4403NGLECEnSqFGj5Pf748YjHA5r27ZtcePR0tKihoaGWJ9NmzYpGo2qtLS0H64iNZw4cUJOZ/yvsLS0NEWjUUmMRUpI9F26PbV27VrjdrvN6tWrzd69e82dd95p8vLy4p5QQO/MnTvXeL1e89vf/ta8++67se3EiROxPnfddZcpLi42mzZtMjt37jSBQMAEAoFY+6lHW6dOnWp27dplNmzYYIYMGcKjrX3gg0/xGMNY9Kft27eb9PR08/DDD5v9+/ebp556ymRnZ5tf/OIXsT5Lly41eXl55le/+pX505/+ZD772c+e9tHWa6+91mzbts288sorZvTo0Tza2kNVVVXm0ksvjT1m/D//8z9m8ODB5lvf+lasD2OR3JIuoBhjzH/+53+a4uJi43K5zPXXX2+2bt2a6JJSiqTTbqtWrYr1OXnypPnXf/1Xc8kll5js7GzzL//yL+bdd9+NO8/bb79tpk2bZrKysszgwYPNN77xDdPZ2dnPV5N6/jGgMBb969lnnzXjxo0zbrfbjBkzxvzkJz+Ja49Go+b+++83Pp/PuN1uM2XKFLNv3764PkeOHDEzZswwubm5xuPxmC9/+cvm2LFj/XkZSS8cDpu7777bFBcXm8zMTHPZZZeZb3/723GPzjMWyc1hzAeW3QMAALBAUt2DAgAABgYCCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABY5/8AbY4u2Rsw26QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import soccer_env\n",
    "\n",
    "env = gym.make(\"SoccerEnv\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.FlattenObservation(env)\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "frame = env.render()\n",
    "plt.imshow(frame)\n",
    "\n",
    "print(state, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5fdfe7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1026,  0.0000,  0.0000,  0.0000,  0.6026,  0.1000,  0.0000,  0.0000,\n",
       "         -0.2051,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STATE_SCALE = np.array([50, 30, 30, 30, 50, 30, 30, 30, 50, 30, 30, 30])\n",
    "STATE_SCALE = torch.tensor(np.ones_like(STATE_SCALE) / STATE_SCALE, device=device).unsqueeze(0)\n",
    "\n",
    "def preprocess_state(state, label):\n",
    "    state = state.clone().detach()\n",
    "\n",
    "    state *= STATE_SCALE\n",
    "    if label == \"Player1\":\n",
    "        state[:, 0:2] -= state[:, 4:6]\n",
    "        state[:, 8:10] -= state[:, 4:6]\n",
    "    if label == \"Player2\":\n",
    "        state[:, 0:2] -= state[:, 8:10]\n",
    "        state[:, 4:6] -= state[:, 8:10]\n",
    "    \n",
    "    return state\n",
    "\n",
    "preprocess_state(torch.tensor(state, device=device).unsqueeze(0), \"Player1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a2f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions, label):\n",
    "        super(DQN, self).__init__()\n",
    "        d_model = 5\n",
    "        self.layer1 = nn.Linear(n_observations, d_model)\n",
    "        self.layer2 = nn.Linear(d_model, d_model)\n",
    "        self.layer3 = nn.Linear(d_model, n_actions)\n",
    "\n",
    "        self.label = label\n",
    "\n",
    "    # 최적화 중에 다음 행동을 결정하기 위해서 하나의 요소 또는 배치를 이용해 호촐됩니다.\n",
    "    # ([[left0exp,right0exp]...]) 를 반환합니다.\n",
    "    def forward(self, x):\n",
    "        x = preprocess_state(x, self.label)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08dc7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE는 리플레이 버퍼에서 샘플링된 트랜지션의 수입니다.\n",
    "# GAMMA는 이전 섹션에서 언급한 할인 계수입니다.\n",
    "# EPS_START는 엡실론의 시작 값입니다.\n",
    "# EPS_END는 엡실론의 최종 값입니다.\n",
    "# EPS_DECAY는 엡실론의 지수 감쇠(exponential decay) 속도 제어하며, 높을수록 감쇠 속도가 느립니다.\n",
    "# TAU는 목표 네트워크의 업데이트 속도입니다.\n",
    "# LR은 ``AdamW`` 옵티마이저의 학습율(learning rate)입니다.\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.8\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "class ModelSettings:\n",
    "    def __init__(self, model, n_observations, action_space, **kwargs):\n",
    "        self.action_space = action_space\n",
    "        n_actions = action_space.n\n",
    "\n",
    "        self.policy_net = model(n_observations, n_actions, **kwargs).to(device)\n",
    "        self.target_net = model(n_observations, n_actions, **kwargs).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "        self.memory = ReplayMemory(10000)\n",
    "\n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state, eval=False):\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        if eval:\n",
    "            eps_threshold = EPS_END\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
    "                # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
    "                # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[self.action_space.sample()]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\n",
    "        # 전환합니다.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다\n",
    "        # (최종 상태는 시뮬레이션이 종료 된 이후의 상태)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
    "        # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
    "        # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
    "        # max(1).values로 최고의 보상을 선택하십시오.\n",
    "        # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "        # 기대 Q 값 계산\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch.squeeze(1)\n",
    "\n",
    "        # Huber 손실 계산\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 모델 최적화\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 변화도 클리핑 바꿔치기\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        target_net_state_dict = self.target_net.state_dict()\n",
    "        policy_net_state_dict = self.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "setting1 = ModelSettings(DQN, n_observations, env.action_space[0], label=\"Player1\")\n",
    "setting2 = ModelSettings(DQN, n_observations, env.action_space[1], label=\"Player2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "612eb6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "\n",
    "    ax = plt.subplot(2, 2, 1)\n",
    "\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "\n",
    "    ax.plot(durations_t.numpy())\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        ax.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42229887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0217, -0.0228], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep((action1\u001b[38;5;241m.\u001b[39mitem(), action2\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     frame_bgr \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n\u001b[0;32m     17\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnv\u001b[39m\u001b[38;5;124m'\u001b[39m, frame_bgr)\n",
      "File \u001b[1;32mc:\\Users\\82109\\miniconda3\\lib\\site-packages\\gymnasium\\core.py:337\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\82109\\miniconda3\\lib\\site-packages\\gymnasium\\core.py:337\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\82109\\miniconda3\\lib\\site-packages\\gymnasium\\wrappers\\common.py:409\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m     )\n\u001b[1;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\82109\\miniconda3\\lib\\site-packages\\gymnasium\\core.py:337\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\82109\\miniconda3\\lib\\site-packages\\gymnasium\\wrappers\\common.py:303\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AI\\popo_AI\\soccer\\soccer_env.py:256\u001b[0m, in \u001b[0;36mSoccerEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHEIGHT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGREEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Draw ground\u001b[39;00m\n\u001b[0;32m    259\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(frame,\n\u001b[0;32m    260\u001b[0m               (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(HEIGHT \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m PPM)),\n\u001b[0;32m    261\u001b[0m               (\u001b[38;5;28mint\u001b[39m(WIDTH), \u001b[38;5;28mint\u001b[39m(HEIGHT)),\n\u001b[0;32m    262\u001b[0m               BLACK,\n\u001b[0;32m    263\u001b[0m               \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\82109\\miniconda3\\lib\\site-packages\\numpy\\core\\numeric.py:344\u001b[0m, in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[0;32m    342\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    343\u001b[0m a \u001b[38;5;241m=\u001b[39m empty(shape, dtype, order)\n\u001b[1;32m--> 344\u001b[0m \u001b[43mmultiarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyto\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munsafe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mcopyto\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # 환경과 상태 초기화\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action1 = setting1.select_action(state)\n",
    "        action2 = setting2.select_action(state, eval=True)\n",
    "        observation, reward, terminated, truncated, info = env.step((action1.item(), action2.item()))\n",
    "\n",
    "        if True:\n",
    "            frame = env.render()\n",
    "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imshow('Env', frame_bgr)\n",
    "            cv2.waitKey(1)\n",
    "            #ax = plt.subplot(2, 2, 2)\n",
    "            #ax.imshow(frame)\n",
    "            #plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "            #if is_ipython:\n",
    "            #    display.display(plt.gcf())\n",
    "            #    display.clear_output(wait=True)\n",
    "\n",
    "        #if t == 300:\n",
    "        #    truncated = True\n",
    "\n",
    "        d1 = torch.norm(state[0][0:2] - state[0][4:6]).item()\n",
    "        d2 = torch.norm(state[0][0:2] - state[0][8:10]).item()\n",
    "        \n",
    "        d3 = np.linalg.norm(observation[0:2] - observation[4:6])\n",
    "        d4 = np.linalg.norm(observation[0:2] - observation[8:10])\n",
    "\n",
    "        reward1 = int(info[\"winner\"] == \"Player1\") * 10\n",
    "        reward2 = int(info[\"winner\"] == \"Player2\") * 10\n",
    "        reward1, reward2 = (reward1 - reward2, reward2 - reward1)\n",
    "        reward1 += (d1 - d3)\n",
    "        reward2 += (d2 - d4)\n",
    "        reward = torch.tensor([reward1, reward2], device=device)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # 메모리에 변이 저장\n",
    "        setting1.memory.push(state, action1, next_state, reward[0].view(1, 1))\n",
    "        setting2.memory.push(state, action2, next_state, reward[1].view(1, 1))\n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "        # (정책 네트워크에서) 최적화 한단계 수행\n",
    "        #setting1.optimize_model()\n",
    "        #setting2.optimize_model()\n",
    "\n",
    "        ## 목표 네트워크의 가중치를 소프트 업데이트\n",
    "        ## θ′ ← τ θ + (1 −τ )θ′\n",
    "        #setting1.update_target_net()\n",
    "        #setting2.update_target_net()\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            print(reward)\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52b17bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (layer1): Linear(in_features=12, out_features=5, bias=True)\n",
       "  (layer2): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (layer3): Linear(in_features=5, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setting1.policy_net.load_state_dict(torch.load('model1-7.pth', map_location='cpu'))\n",
    "setting1.policy_net.eval()\n",
    "\n",
    "setting2.policy_net.load_state_dict(torch.load('model2-8.pth', map_location='cpu'))\n",
    "setting2.policy_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62e42e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
